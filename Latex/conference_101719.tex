\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[vietnamese]{babel}
\usepackage{amsmath}
\usepackage{float}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{hyperref}
\setlength{\parindent}{0pt}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage[backend=bibtex,style=ieee]{biblatex}
\addbibresource{ref.bib}

\begin{document}

\title{Dự báo giá cổ phiếu ngân hàng Việt Nam bằng các mô hình Thống kê, Máy học và Học sâu\\}
\author{
    \IEEEauthorblockN{
        \begin{minipage}{\textwidth}
            \centering
            \begin{minipage}{0.3\textwidth}
                \centering
                1\textsuperscript{st} Trần Ngọc Tố Như\\
                \textit{Khoa Hệ thống thông tin,}\\
                \textit{Trường Đại học Công nghệ thông tin-ĐHQG}\\
                21520385@gm.uit.edu.vn
            \end{minipage}
            \hfill
            \begin{minipage}{0.3\textwidth}
                \centering
                2\textsuperscript{nd} Lê Thùy Dương\\
                \textit{Khoa Hệ thống thông tin,}\\
                \textit{Trường Đại học Công nghệ thông tin-ĐHQG}\\
                21520203@gm.uit.edu.vn
            \end{minipage}
            \hfill
            \begin{minipage}{0.3\textwidth}
                \centering
                3\textsuperscript{rd} Trần Thanh Huy\\
                \textit{Khoa Hệ thống thông tin,}\\
                \textit{Trường Đại học Công nghệ thông tin-ĐHQG}\\
                21522170@gm.uit.edu.vn
            \end{minipage}
        \end{minipage}
        \\[1.5em] % khoảng cách giữa hai hàng
        \begin{minipage}{\textwidth}
            \centering
            \begin{minipage}{0.45\textwidth}
                \centering
                4\textsuperscript{th} Mai Trần Khương Duy\\
                \textit{Khoa Hệ thống thông tin,}\\
                \textit{Trường Đại học Công nghệ thông tin-ĐHQG}\\
                21521998@gm.uit.edu.vn
            \end{minipage}
            \hfill
            \begin{minipage}{0.45\textwidth}
                \centering
                5\textsuperscript{th} Vũ Tiến Linh\\
                \textit{Khoa Hệ thống thông tin,}\\
                \textit{Trường Đại học Công nghệ thông tin-ĐHQG}\\
                19521760@gm.uit.edu.vn
            \end{minipage}
        \end{minipage}
    }
}

\maketitle

\begin{abstract}
Việc tích hợp công nghệ thông tin vào các khía cạnh khác nhau của cuộc sống, bao gồm kinh tế, y tế và thương mại, ngày càng trở nên phổ biến. Nhất là trong các lĩnh vực được quan tâm đặc biệt, nhu cầu ứng dụng mạnh mẽ công nghệ thông tin ngày càng tăng cao, đặt ra thách thức cho những người làm trong ngành công nghệ thông tin. Một trong những bài toán được quan tâm hiện nay là việc dự đoán giá cổ phiếu của các tổ chức ngân hàng. Báo cáo này tập trung vào dự đoán giá cổ phiếu của ba ngân hàng: BIDV, VCB và MBB, sử dụng nhiều thuật toán thuộc nhiều loại khác nhau, bao gồm các thuật toán học sâu (MICN, CNN-LSTM), thuật toán máy học (XGBoost, Linear Regression với CalendarFourier, DeterministicProcess) và thuật toán thống kê (Linear Regression, Holt-Winter) để dự đoán, đồng thời so sánh, đánh giá kết quả khi sử dụng những thuật toán nêu trên.
\end{abstract}

\begin{IEEEkeywords}
Stock, Linear Regression, ARIMA, RNN, GRU, LSTM, SARIMAX, Boosting Model, Stacking Model, ResNet, CNN-LSTM
\end{IEEEkeywords}

\section{Giới thiệu}
Trong bất kỳ quốc gia nào, thị trường trứng khoán đóng vai trò quan trọng trong nền kinh tế của mỗi quốc gia, thị trường chứng khoán Việt Nam xem là tương đối trẻ so với các quốc gia trên toàn cầu. Sự bùng nổ về trí tuệ nhân tạo và học máy đã thúc đẩy sự quan tâm của các nhà đầu tư cá nhân, tổ chức vào việc tận dụng các công nghệ này để dự đoán thị trường chứng khoán nói chung và giá cổ phiếu nói riêng tại Việt Nam. 

Do đó trong nghiên cứu này, nhóm sẽ đi sâu vào việc áp dụng các thuật toán thống kê, máy học, học sâu khác nhau như Linear Regression, Holt-Winters, ARIMA, Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), Extreme Gradient Boosting (XGBoost), Linear Regression áp dụng CalendarFourier, DeterministicProcess, Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM) và Multi-Channel Neural Networks (MICN), đồng thời cũng đánh giá hiệu suất của các thuật toán nêu trên. 

Trong nghiên cứu này, nhóm chọn ngành ngân hàng là vì nó có tác động trực tiếp đến nền kinh tế của đất nước. Ngoài ra, cổ phiếu ngân hàng còn đại diện cho một ngành ổn định với vốn hóa thị trường đáng kể trên sàn giao dịch chứng khoán Việt Nam. Nghiên cứu tập trung vào dự đoán giá cổ phiếu cho ba ngân hàng lớn được niêm yết trên thị trường chứng khoán Việt Nam hiện nay là BIDV, VCB và MBB.

\section{CÁC CÔNG TRÌNH NGHIÊN CỨU LIÊN QUAN}
Trong những năm qua, việc xác định dữ liệu chứng khoán rất phức tạp, do đó có rất nhiều phương pháp dự đoán chuỗi thời gian đã được đề xuất. Điển hình là theo \cite{predictStockCNNLSTM}, nhóm tác giả đã sử dụng CNN-LSTM để đưa ra dự đoán cổ phiếu của một công ty trên thị trường chứng khoán nếu biết được những thông tin trước đó. Độ chính xác của mô hình CNN-LSTM được đánh giá là cao ngay cả khi huấn luyện trên dữ liệu thị trường chứng khoán theo thời gian thực. Bằng cách xác định các tính năng từ dữ liệu chứng khoán và chuyển đổi chúng thành tensor (dữ liệu nhiều hơn 2 chiều), ta có thể có được các tính năng và sau đó gửi nó tới mạng thần kinh LSTM để tìm ra các mẫu và từ đó dự đoán cổ phiếu thị trường trong một khoảng thời gian nhất định.

Mặt khác, Poongodi M, Vijayakumar V và Naveen Chilamkurti đã thu thập dữ liệu về bitcoin blockchain từ ngày 28 tháng 4 năm 2013 đến ngày 31 tháng bảy năm 2017 được có sẵn công khai trên
\href{https://coinmarketcap.com}{https://coinmarketcap.com} và áp dụng mô hình ARIMA để dự đoán giá bitcoin \cite{predictBitcoinARIMA}.

Các nghiên cứu trước đây đã khám phá các phương pháp khác nhau để giải quyết nhiệm vụ khó khăn của việc dự đoán giá cổ phiếu. \cite{randomForestAndLSTM} Nghiên cứu đã đề xuất một mô hình kết hợp bằng cách kết hợp mạng nơ-ron hồi quy (RNN) với Random Forest, thể hiện sự cải thiện đáng kể trong dự báo giá cổ phiếu.

Trong một bài báo khác, Xiwen Jin và Chaoran Yi đã kết luận rằng LSTM và GRU cho kết quả tương đối tốt hơn và Random Forest là tệ nhất. Điểm R2 cho các mô hình khác nhau mà các tác giả đã phân tích: LSTM 0.84, GRU 0.86, mô hình Hồi quy Random Forest 0.51, mô hình Hồi quy XGBoost 0.69, Hồi quy Tuyến tính 0.73 và mô hình Hồi quy LGBM 0.72. Từ đó có thể thấy rằng XGBoost và Random Forest có hiệu suất không tốt bằng  các mô hình LSTM và GRU \cite{comparationLinearRegression}.

MICN mang lại sự cải thiện 17,2\% và 21,6\% cho phương pháp đa biến. Sử dụng kết hợp CNN và Transformers để hướng tới mục tiêu sử dụng thông tin tổng thể của đầu vào một cách hiệu quả. Trước tiên là trích xuất đặc trưng cục bộ của dữ liệu, sau đó lập mô hình mối tương quan toàn cầu trên cơ sở này \cite{MICN}.

Ahmad M. Awajan, Mohd Tahir Ismail, Sadam Alwadi đã triển khai một phương pháp kết hợp dựa trên Empirical Mode Decomposition và Holt-Winter (EMD-HW) để dự đoán thị trường chứng khoán. Dữ liệu thị trường chứng khoán được phân rã bằng phương pháp EMD thành các Intrinsic Mode Functions (IMFs) và các thành phần dư còn lại. Tất cả các thành phần được dự báo bằng kỹ thuật Holt - Winter. Các giá trị dự báo sẽ được tổng hợp để có giá trị dự đoán cho thị trường chứng khoán. Điểm mạnh của EMD-HW này nằm ở khả năng dự đoán các chuỗi thời gian không ổn định và phi tuyến mà không cần sử dụng bất kỳ phương pháp biến đổi nào \cite{forecastingHoltWinter}.
\section{MATERIALS}
\subsection{BỘ DỮ LIỆU}
Lịch sử giá cổ phiếu của 3 ngân hàng: Ngân hàng Thương mại cổ phần Đầu tư và Phát triển Việt Nam (BIDV), ngân hàng thương mại cổ phần Ngoại thương Việt Nam (VCB), ngân hàng Thương mại cổ phần Quân đội (MBB). Dữ liệu được lấy nằm trong khoảng thời gian từ ngày 1 tháng 1 năm 2018 đến ngày 1 tháng 6 năm 2024. Mỗi bộ dữ liệu chứa khoảng 1555 dòng và bao gồm 7 thuộc tính: Date, Price, Open, High, Low, Vol, Change.\\
Trong đó: 
\begin{itemize}
    \item Date: Ngày giao dịch 
    \item Price: Giá trị cuối cùng của cổ phiếu tại giờ đóng cửa
    \item Open: Giá mở cửa của cổ phiếu tại ngày giao dịch
    \item High: Giá cao trị cao nhất mà cổ phiếu đạt đến trong ngày 
    \item Low: Giá thấp nhất mà cổ phiếu đạt được trong ngày
    \item Vol: Khối lượng giao dịch của cổ phiếu trong ngày (đơn vị: triệu cổ phiếu)
    \item Change: Sự chênh lệch của giá đóng cửa so với giá đóng cửa của ngày trước đó (giá trị %)
\end{itemize}


\subsection{THỐNG KÊ MÔ TẢ}
\begin{table}[htbp]
\caption{BIDV, VCB, MBB’s Descriptive Statistics}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{} & \textbf{\textit{BIDV}} & \textbf{\textit{VCB}} & \textbf{\textit{MBB}} \\
\hline
Count & 1598 & 1598 & 1598 \\
\hline
Mean & 33466.789 & 67714.813 & 15575.501 \\
\hline
Std & 7261.683 & 14916.299 & 5360.553 \\
\hline
Min & 16531.4 & 35483 & 7206.6 \\
\hline
25\% & 28166.65 & 55687.75 & 10936.525 \\
\hline
50\% & 32375.3 & 67731 & 14605.7 \\
\hline
75\% & 38500 & 78577 & 18786.2 \\
\hline
Max & 54400 & 97400 & 28666.7 \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Statistic/bidv_boxplot.png}
    \captionof{figure}{Box Plot of BIDV stock price (2018 - 2024)}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Statistic/bidv_histogram.png}
    \captionof{figure}{Histogram of BIDV Stock Price (2018 - 2024)}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Statistic/vcb_boxplot.png}
    \captionof{figure}{Box Plot of VCB Stock Price (2018 - 2024)}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Statistic/vcb_histogram.png}
    \captionof{figure}{Histogram of VCB Stock Price (2018 - 2024)}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Statistic/mbb_boxplot.png}
    \captionof{figure}{Box Plot of MB Bank Stock Price (2018 - 2024)}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Statistic/mbb_histogram.png}
    \captionof{figure}{Histogram of MB Bank Stock Price (2018 - 2024)}
    \label{fig:image2}
\end{minipage}



\section{METHODS}
\subsection{ARIMA}
Mô hình ARIMA là là sự kết hợp của quá trình từ hồi quy (Auto Regression – AR), quá trình trung bình trượt (Moving Average - MA) và Tính hợp sai phân (Integrated – I). Mô hình ARIMA chỉ hoạt động tốt nếu dữ liệu phụ thuộc nhiều vào thời gian và trong chuỗi dữ liệu dừng. Những dữ liệu dạng ngẫu nhiên thường ít hoạt động tốt đối với mô hình ARIMA. Mô hình ARIMA cần bộ dữ liệu có tính dừng, nếu không có tính dừng, cần tích hợp sai phân để làm bộ dữ liệu có tính dừng.\\
\textbf{Chuỗi dừng:} Một chuỗi thời gian có tính dừng  là một chuỗi các giá trị Mean, Variance, Autocorrelation không thay đổi theo thời gian và nó không bao hàm yếu tố xu hướng.\\
\textbf{Mô hình ARIMA không có tính mùa vụ:} 
Mô hình ARIMA được biểu diễn với ký hiệu tiêu chuẩn được sử dụng là ARIMA(p,d,q) với:
\begin{itemize}
    \item I(d) Integrated so sánh sự khác nhau giữa d quan sát, hiệu giữa giá trị hiện tại và d giá trị trước đó. Quá trình sai phân được thực hiện: \\
    Sai phân bậc 1:\(I(1) = \Delta y_t = y_t - y_{t-1}\) \\
    Sai phân bậc 2:\(I(2) = \Delta^2 y_t = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})\) \\
    Sai phân bậc d được ký hiệu là I(d)
    \item AR(p) Autoregression: là quá trình tìm mối quan hệ giữa dữ liệu hiện tại và p dữ liệu quá khứ trước đó gọi là (Lag) được biểu diễn với công thức. 
    \[y_t = a_0 + a_1 y_{t-1} + a_2 y_{t-2} + \cdots + a_p y_{t-p} + \epsilon_t\]
    Điều kiện dừng của việc chọn \( p \): \(\sum_{i=0}^p a_i < 1\)
    \item MR(q) Moving Average là quá trình tìm mối quan hệ giữa dữ liệu hiện tại và q phần lỗi quá khứ trước đó được biểu diễn với công thức
    \[
    y_t = \beta_0 + \beta_1 \epsilon_{t-1} + \beta_2 \epsilon_{t-2} + \cdots + \beta_q \epsilon_{t-q} + \mu_t
    \]
    Điều kiện dừng của việc chọn \( q \):\(\sum_{i=0}^q \beta_i < 1\)
\end{itemize}





\subsection{Linear Regression}
Hồi quy tuyến tính là phương pháp thống kê để mô hình hóa mối quan hệ giữa một biến phụ thuộc với một hoặc nhiều biến độc lập. Thuật toán này sẽ giả định mối quan hệ tuyến tính giữa những yếu tố này và tìm cách tìm ra đường thẳng tốt nhất để dự đoán giá trị của biến phụ thuộc dựa trên các biến độc lập sao cho sự sai lệch giữa kết quả dự đoán và giá trị thực tế của biến phụ thuộc là nhỏ nhất. Mô hình hồi quy tuyến tính có hai loại: Mô hình hồi quy tuyến tính đơn biến và hồi quy tuyến tính đa biến. Mô hình hồi quy tuyến tính đơn biến chỉ có một biến độc lập còn mô hình hồi quy tuyến tính đa biến là mô hình hồi quy tuyến tính có nhiều hơn một biến độc lập. Mô hình hồi quy tuyến tính đơn biến là một trường hợp đặc biệt của mô hình hồi quy tuyến tính đa biến. Vậy nên mô hình hồi quy tuyến tính có công thức chung như sau:
\cite{linearRegressionMethod} 
\[Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_kX_k+\varepsilon\]
Trong đó:
\begin{itemize}
    \item Y là biến phụ thuộc (biến mục tiêu)
    \item $X_1$, $X_2$,..., $X_k$ là các biến độc lập (biến giải thích)
    \item $\beta_0$ là hệ số giao nhau (hệ số chặn)
    \item $β_1$, $β_2$, ..., $β_k$ là các hệ số hồi quy cho các biến độc lập
    \item $\varepsilon$ là sai số (thành phần chưa giải thích bởi mô hình)
\end{itemize}
Với mô hình hồi quy tuyến tính đa biến, chúng ta có thể đánh giá tác động của từng biến độc lập lên biến phụ thuộc và dùng mô hình này để dự đoán giá trị của biến phụ thuộc dựa trên giá trị của các biến độc lập.

\subsection{Holt Winter}
Holt Winters (HW) là một phương pháp mở rộng của phương pháp Holt, được áp dụng khi dữ liệu có xu hướng và có tính mùa vụ. Tùy thuộc vào loại mùa vụ, Holt Winters có thể là “additive”  hoặc “multiplicative”, xác định dựa vào sự dao động theo thời gian. Trong cả hai phiên bản, các dự báo sẽ phụ thuộc vào ba thành phần của một chuỗi thời gian có tính mùa vụ: mức độ, xu hướng và hệ số mùa vụ của nó. \cite{HoltWinter1} 

Exponential smoothing (Làm mịn hàm mũ) là một kỹ thuật được sử dụng để làm mịn dữ liệu chuỗi thời gian bằng cách gán trọng số giảm dần theo hàm mũ cho các quan sát trong quá khứ, do đó làm giảm ảnh hưởng của các điểm dữ liệu cũ hơn lên kết quả tổng thể được làm mịn.

Có ba biến thể chính của Exponential Smoothing: Single Exponential Smoothing, Double Exponential Smoothing, và Triple Exponential Smoothing.

Công thức cụ thể cho Single Exponential Smoothing \cite{HoltWinter2}:
\begin{align*}
S_t = \alpha \cdot X_t + (1 - \alpha) \cdot S_{t-1}
\end{align*}
Trong đó:
\begin{itemize}
    \item $X_t$ là giá trị thực tại thời điểm t.
    \item $S_t$ là ước tính làm mịn của mức độ vào cuối kỳ t.
    \item $S_{t-1}$ là ước tính làm mịn của mức độ vào cuối kỳ t-1.
    \item $\alpha$ là tham số làm mịn, thường nằm trong khoảng từ 0 tới 1.
\end{itemize}

Công thức cụ thể cho Double Exponential Smoothing:
\begin{align*}
S_t = \alpha \cdot y_t + (1 - \alpha) \cdot (S_{t-1} + b_{t-1}) 0 < α < 1
\\
b_t = \gamma \cdot (S_t - S_{t-1}) + (1 - \gamma) \cdot b_{t-1} 0 < γ < 1 
\end{align*}
Trong đó:
\begin{itemize}
\item $b_t$ là ước tính làm mịn của tốc độ tăng trưởng trung bình vào cuối kỳ t.
\item $\gamma$ là tham số làm mịn cho xu hướng, thường nằm trong khoảng từ 0 tới 1.
\item $b_{t-1}$ là ước tính làm mịn trước đó của xu hướng vào cuối kỳ t-1.
\item $S_t$ là ước tính làm mịn của mức độ vào cuối kỳ t.
\item $S_{t-1}$ là ước tính làm mịn của mức độ vào cuối kỳ t-1.
\end{itemize}

Triple Exponential Smoothing mở rộng từ Double Exponential Smoothing bao gồm thành phần mùa vụ để xử lý chuỗi thời gian có tính mùa vụ. Công thức cụ thể của Triple Exponential Smoothing: \cite{HoltWinter3}
\begin{align*}
    S_t = \alpha \frac{y_t}{I_{t-L}} + (1-\alpha)(S_{t-1}+b_{t-1}) 
\end{align*}
Trong đó:
\begin{itemize}
\item $S_t$ là quan sát đã được làm mịn tại thời điểm (t).
\item $y_t$ là quan sát thực tế tại thời điểm (t).
\item $I_{t-L}$ là chỉ số mùa vụ cho cùng mùa trong năm trước.
\item $\alpha$ là hằng số cần được ước tính.
\item $b_t$ là yếu tố xu hướng tại thời điểm (t).
\end{itemize}
\begin{align*}
b_t = \gamma (S_t - S_{t-1}) + (1 - \gamma)b_{t-1} & &
\end{align*}
Trong đó:
\begin{itemize}
\item $(b_t)$ thể hiện thành phần xu hướng.
\item $(\gamma)$ là một hằng số khác cần được ước tính.
\end{itemize}
\begin{align*}
 I_t = \beta \frac{y_t}{S_t} + (1 - \beta) I_{t-L} & &
\end{align*}
Trong đó:
\begin{itemize}
\item $(I_t)$ đại diện cho chỉ số mùa vụ tại thời điểm (t).
\item $(\beta)$ là một hằng số khác cần được ước tính.
\end{itemize}
\begin{align*}
F_{t+m} = (S_t + m b_t) I_{t-L+m} & &
\end{align*}
Trong đó:
\begin{itemize}
\item $(F_{t+m})$ là dự báo tại (m) kỳ tiếp theo.
\item $(m)$ là số kỳ trong tương lai.
\end{itemize}

\subsection{Linear Regression áp dụng Calendar Fourier, Deterministic}
Hồi quy tuyến tính áp dụng CalendarFourier được sử dụng để mô hình hóa tính thời vụ bằng chuỗi Fourier để xử lý các yếu tố có tính chu kỳ trong dữ liệu. Fourier sử dụng các hàm sóng sin và cos để biểu diễn một hàm tuần hoàn. Trong hồi quy tuyến tính sử dụng các hàm sóng này như các biến độc lập trong mô hình hồi quy tuyến tính. Hữu ích để nắm bắt các mô hình theo mùa phức tạp.
\[Y = \beta_0 + \beta_1 \cos(\omega t) + \beta_2 \sin(\omega t) + \ldots + \beta_n \cos(k \omega t) + \beta_{n+1} \sin(k \omega t)\]
Trong đó:
\begin{itemize}
    \item \(\omega = \frac{2\pi}{T}\) là tần số góc, trong đó \(T\) biểu thị khoảng thời gian.
    \item \(k\) là số lần lặp lại của chuỗi Fourier.
\end{itemize}

Hồi quy tuyến tính áp dụng DeterministicProcess là một phương pháp để tạo các mô hình hóa yếu tố xác định trong dữ liệu chuỗi thời gian. DeterministicProcess kết hợp các hàm thời gian tuyến tính hoặc đa thức với các thành phần Fourier. \cite{LRDeterministicProcess}
\[Y = \beta_0 + B_1 X_1 t + \beta_2 X_2 t + \ldots + \beta_n X_n t\]
Trong đó:
\begin{itemize}
    \item \(Y\) là biến phụ thuộc.
    \item \(\beta_0\) là số hạng bị chặn hoặc số hạng không đổi.
    \item \(B_1, \beta_2, \ldots, \beta_n\)là các hệ số gắn với các biến độc lập tương ứng \(X_1, X_2, \ldots, X_n\).
    \item \(t\) là biến độc lập.
\end{itemize}

Biểu diễn tổng quát mô hình hồi quy tuyến tính áp dụng CalendarFourier, DeterministicProcess
\[
Y = \beta_0 + \sum_{i=1}^{n} \beta_i X_i + \sum_{j=1}^{m} \left( \alpha_j \cos\left(\frac{2 \pi k_j t}{T}\right) + \gamma_j \sin\left(\frac{2 \pi k_j t}{T}\right) \right) + \epsilon
\]
Trong đó:
\begin{itemize}
    \item \(Y\) là biến phụ thuộc.
    \item \(X_i\) là các biến đặc trưng.
    \item \(\cos\left(\frac{2 \pi k_j t}{T}\right)\), \(\sin\left(\frac{2 \pi k_j t}{T}\right)\) là các biến đặc trưng fourier
    \item \(\alpha_j\), \(\gamma_j\) là các hệ số hồi quy có thành phần fourier
\end{itemize}

\subsection{XGBoost}
XGBoost (eXtreme Gradient Boosting) là một mô hình máy học dựa trên  Gradient Boosting nhưng được tối ưu hóa và xử lý song song giúp cải thiện đáng kể thời gian đào tạo mô hình. \cite{XGBoost}

XGBoost thực hiện tìm nhiều cây quyết định khác nhau đơn giản sau đó dùng kết quả cây quyết định và độ lỗi trước đó làm đầu vào cho bước tìm cây quyết định tiếp theo. Sau số lần lặp hoặc ngưỡng chấp nhận nhất định thì dừng thuật toán. \cite{XGBoost}

Ví dụ với tập dữ liệu n dòng và m thuộc tính, ta có:
\[
D = \{(x_i, y_i) \mid i = 1, 2, \ldots, n, \quad x_i \in \mathbb{R}^m, \quad y_i \in \mathbb{R}\}
\]
Một mô hình tập hợp cây dựa trên K cây nhỏ để dự đoán đầu ra như sau:
\[
\hat{y}_i = \phi(x_i) = \sum_{k=1}^{K} f_k(x_i), \quad f_k \in \mathcal{F}
\]
Trong đó
\begin{itemize}
    \item \(\mathcal{F} = \{ f(x) = \omega_{q(x)} \mid q: \mathbb{R}^m \rightarrow T, \quad \omega \in \mathbb{R}^T \}\) ứng với các cây hồi quy
    \item \(q\) đại diện cho cấu trúc mỗi cây với số lá tương ứng \(\rightarrow T\). 
    \item Mỗi \(f_k\) ứng với một cấu trúc cây độc lập \(q\) và trọng số lá \(\omega\)
    \item \(\omega_i\) đại diện cho lá thứ i. Đối với từng dòng dữ liệu, sử dụng quy tắc cây quyết định để phân loại và tính toán cộng điểm các lá tương ứng.
\end{itemize}
Để tối ưu qua từng cây được sử dụng trong mô hình, tối ưu hóa mục tiêu điều chỉnh như sau:
\[
L(\phi) = \sum_{i} l(\hat{y}_i, y_i) + \sum_{k} \Omega(f_k)
\]
Trong đó: \(\Omega(f) = \gamma T + \frac{1}{2} \lambda \|\omega\|^2\) \\
Với: 
\begin{itemize}
    \item \( l \): Hàm mất mát.
    \item \( \Omega \): Hàm tính độ phức tạp.
    \item \( T \): Số lượng lá.
    \item \( \omega \): Trọng số lá.
    \item \( \gamma \): Tham số điều chỉnh số lượng lá.
    \item \( \lambda \): Tham số điều chỉnh chuẩn \( L2 \) của các điểm số lá.
\end{itemize}
Việc tính toán \(L(\phi)\) bao gồm các tham số được huấn luyện bổ sung, để dự đoán \( \hat{y}_i^{(t)} \) ở dòng dữ liệu thứ \( i \) và bước lặp thứ \( t \), cần thêm \( f_t \) để tối ưu hóa mục tiêu.
\[
L^{(t)} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)
\]
\[
L^{(t)} \approx \sum_{i=1}^{n} \left[ l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right] + \Omega(f_t)
\]
Với:
\begin{itemize}
    \item \(\)

    \item \(g_i = {\partial_\hat{y}_i^{(t-1)}} l(y_i, \hat{y}^{(t-1)})\): Gradient descent bậc 1.
    \item \( h_i = {\partial^2_\hat{y}_i^{(t-1)}} l(y_i, \hat{y}^{(t-1)} \): Gradient descent bậc 2.
\end{itemize}

\subsection{RNN}
RNN (Mạng nơ-ron hồi quy) is a neural network that combines variable-length input data with a hidden state that depends on previous time steps to produce output data. Through the connections among hidden units associated with the time delay, the model can retain information about the past, enabling it to discover temporal correlations between events that are far away from each other in the data.\cite{RNN}\\
RNN (Mạng nơ-ron hồi quy) là một mạng nơ-ron kết hợp dữ liệu đầu vào có độ dài biến đổi trong khi có trạng thái ẩn, trạng thái này phụ thuộc vào các bước thời gian trước đó để tạo ra dữ liệu đầu ra. Thông qua các kết nối giữa các đơn vị ẩn liên quan đến độ trễ thời gian, mô hình có thể ghi nhớ thông tin từ quá khứ, cho phép nó nắm bắt các tương quan theo thời gian giữa các sự kiện cách xa nhau trong dữ liệu.
Mô hình chuẩn của RNN được minh họa như trong hình dưới đây:\\
\begin{figure}[H]
    \centering
    \begin{minipage}{0.8\linewidth}
    \centering
        \includegraphics[width=\linewidth]{images/RNN.png}
    \caption{Mô hình RNN chuẩn (Nguồn: \cite{DeepLearning})}
    \label{fig7}
    \end{minipage}
\end{figure}
Trạng thái ẩn \( S_t \) tại bước \( t \) được tính dựa trên đầu vào  \( X_t \) tại bước \( t \) và trạng thái ẩn \( S_{t-1} \) tại bước trước:
\[s_{t} = f(U_{x_{t}} + W_{s_{t-1}})\]
Trong đó:
\begin{itemize}
    \item \( s_t \): Trạng thái ẩn tại thời điểm \( t \).
    \item \( x_{t} \): Đầu vào tại thời điểm \( t \).
    \item \( s_{t-1} \): Trạng thái ẩn tại thời điểm \( t-1 \).
    \item \( U \): Ma trận trọng số từ đầu vào đến trạng thái ẩn.
    \item \( W \): Ma trận trọng số từ trạng thái ẩn trước đến trạng thái ẩn hiện tại.
    \item \( f \): Hàm kích hoạt (thường là hàm tanh hoặc sigmoid).
\end{itemize}
\[y_t = \sigma(V_{s_{t}})\]
Trong đó:
\begin{itemize}
    \item \( y_t \): Đầu ra tại thời điểm \( t \).
    \item \( s_{t} \): Trạng thái ẩn tại thời điểm \( t \).
    \item \( V \): Ma trận trọng số từ trạng thái ẩn đến đầu ra.
    \item $\sigma$: Hàm kích hoạt. 
\end{itemize}

\subsection{GRU}
Kyunghyun Cho là người đầu tiên giới thiệu GRU vào năm 2014. Thuật toán dựa trên Recurrent Neural Network (RNN) tương đương với LSTM nhưng có kiến trúc đơn giản hơn. Vấn đề cơ bản của RNN là về biến mất(Vanishing) và bùng nổ(Exploding) xảy ra do phép nhân liên tục trong quá trình Lan truyền ngược theo thời gian(Backpropagation Through Time). GRU giải quyết vấn đề này bằng cách sử dụng các cổng, cụ thể là cổng cập nhật và cổng đặt lại, không giống như LSTM, GRU chỉ bao gồm hai cổng và không duy trì trạng thái tế bào (cell state) bên trong. Thông tin được lưu trữ ở trạng thái tế bào bên trong trong đơn vị tái phát LSTM được tích hợp vào trạng thái ẩn (hidden state) của GRU. \cite{ComparisonGRU&ARIMA}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.8\linewidth}
    \centering
        \includegraphics[width=\linewidth]{images/GRU.png}
    \caption{Kiến trúc của GRU}
    \label{fig8}
    \end{minipage}
\end{figure}
Kiến trúc của GRU:
Reset gate: xác định có bao nhiêu thông tin trong quá khứ phải được giữ lại.
\[r_t = \sigma(W_r x_t + [h_{t-1}, x_t] + b_r)\]
Update gate: xác định lượng thông tin trước đó sẽ bị xóa và kết hợp đầu vào với thông tin cũ.
\[z_t = \sigma(W_z x_t + [h_{t-1}, x_t] + b_z)\]
Candidate hidden state: được Reset gate sử dụng để giữ lại thông tin quan trọng từ quá khứ.
\[\tilde{h}_t = \tanh(W_h [r_t \odot h_{t-1}, x_t] + b_h)\]
Hidden state: là đầu ra của quá trình.
\[h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t\]
Trong đó:
\begin{itemize}
    \item $W_r, W_z, W_h$ là những ma trận trọng số.
    \item $x_t$ là đầu vào tại bước thời gian t.
    \item $h_{t-1}$ là trạng thái ẩn trước đó.
    \item $h_t$ là trạng thái ẩn hiện tại.
\end{itemize}


\subsection{LSTM}
LSTM (Long short-term memory) là một phiên bản cải tiến của RNN có một cấu trúc phức tạp được gọi là ô LSTM (LSTM cell) này trong lớp ẩn (hidden layer) của nó. Khối LSTM có 3 cổng tên là cổng vào (input gate), cổng quên (forget gate) và cổng ra (output gate) như hình vẽ: \cite{LSTM}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.8\linewidth}
    \centering
        \includegraphics[width=\linewidth]{images/LSTM.png}
    \caption{Kiến trúc của LSTM}
    \label{fig9}
    \end{minipage}
\end{figure}
Ba cổng này có tác dụng điều khiển các luồng thông tin đi qua các ô cũng như mạng nơron. \\
Kiến trúc LSTM cần truyền trạng thái ra của ô $C_t$ (cell output state) và đầu ra lớp ẩn $h_t$ (hidden layer output) tới các nơron tiếp theo trong mạng. Để tính các giá trị này tại thời điểm t ta thực hiện các bước theo thứ tự: \\
\textbf{Bước 1:} Tính trạng thái của 3 cổng, trạng thái đầu vào của ô:
\begin{itemize}
    \item Input gate:
    \[i_t = \sigma\left(W_1^i \cdot x_t + W_h^i \cdot h_{t-1} + b_i\right)\]
    \item Forget gate:
    \[f_t = \sigma\left(W_1^f \cdot x_t + W_h^f \cdot h_{t-1} + b_f\right)\]
    \item Output gate:
    \[o_t = \sigma\left(W_1^o \cdot x_t + W_h^o \cdot h_{t-1} + b_o\right)\]
    \item Đầu vào của ô:
    \[\tilde{C}_t = \tanh\left(W_1^C \cdot x_t + W_h^C \cdot h_{t-1} + b_C\right)\]
\end{itemize}
Trong đó: 
\begin{itemize}
    \item $x_t$ là dữ liệu đầu vào tại thời điểm $t$
    \item $h_t$ là đầu ra của lớp ẩn, $h_{t-1}$ là đầu ra của trước đó
    \item $\tilde{C}_t$ là trạng thái đầu vào của ô
    \item $W_1^i, W_1^f, W_1^o, W_1^C$ là các ma trận trọng số để kết nối $x_t$ tới 3 cổng và đầu vào của ô
    \item $W_h^i, W_h^f, W_h^o, W_h^C$ là các ma trận trọng số để kết nối $h_{t-1}$ tới 3 cổng và đầu vào của ô
    \item $b_i, b_f, b_o, b_C$ là các chỉ số bias
    \item $\sigma$ là hàm sigmoid với $\sigma = \frac{1}{1 + \exp(-x)}$
    \item $\tanh$ là hàm hyperbolic tangent với $\tanh = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$
\end{itemize}
\textbf{Bước 2:} Tính toán trạng thái đầu ra của ô:
\[C_t = i_t \cdot \tilde{C}_t + f_t \cdot C_{t-1}\]
Trong đó: 
\begin{itemize}
    \item $C_t$ là trạng thái đầu ra của ô
    \item $C_(t-1)$ là trạng thái đầu ra của trước đó
\end{itemize}
\textbf{Bước 3:} Tính toán đầu ra của lớp ẩn:
\[h_t = o_t \cdot \tanh(C_t)\]

\subsection{CNN-LSTM}
CNN-LSTM (Convolutional Neural Network - Long Short-Term Memory) kết hợp hai kiến trúc mạng nơ-ron quan trọng trong lĩnh vực xử lý dữ liệu hình ảnh và chuỗi. \cite{PredictStockCNNLSTM}\\
CNN-LSTM gồm 2 phần là CNN và LSTM:
\subsubsection{Convolutional Neural Network (CNN)}
\begin{itemize}
    \item CNN có khả năng học các đặc trưng cục bộ của dữ liệu bằng cách áp dụng các bộ lọc (kernels) trượt qua không gian dữ liệu đầu vào.
    \item Các lớp Convolutional và Pooling trong CNN giúp giảm chiều dữ liệu và tạo ra các biểu diễn đặc trưng của dữ liệu đầu vào.
\end{itemize}

\subsubsection{Long Short-Term Memory (LSTM)}
\begin{itemize}
    \item LSTM là một loại mạng nơ-ron tái phát (recurrent neural network - RNN) được thiết kế để xử lý và hiểu các chuỗi dữ liệu có độ dài biến đổi.
    \item The LSTM có ba trạng thái: Forget State, Input State và Output State \cite{ComparasionCNNLSTM}.
\end{itemize}

\subsubsection{CNN-LSTM architecture}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.8\linewidth}
    \centering
        \includegraphics[width=\linewidth]{images/CNNLSTM_Architechture.png}
    \caption{The architecture of the CNN-LSTM model}
    \label{fig10}
    \end{minipage}
\end{figure}
Kiến trúc CNN-LSTM liên quan đến việc sử dụng các lớp CNN để trích xuất các đặc trưng cục bộ từ dữ liệu đầu vào, sau đó đưa các đặc trưng này vào LSTM để mô hình có thể hiểu các mối quan hệ không gian và thời gian giữa các đặc trưng này \cite{PredictStockCNNLSTM}.


\subsection{MICN}
MICN (Multi-Scale Isometric Convolution Network) là mô hình giúp dự đoán hiệu quả và tiết kiệm chi phí cho dự đoán dài hạn bằng cách sử dụng phương pháp tách nhỏ đầu vào và chọn các thuật toán thích hợp để xử lý từ phần nhỏ. Kiến trúc tổng quan của mô hình MICN được thể hiện như sau: \cite{MICN}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.8\linewidth}
    \centering
        \includegraphics[width=\linewidth]{images/MICN.png}
    \caption{Kiến trúc của MICN}
    \label{fig11}
    \end{minipage}
\end{figure}
Mô hình bao gồm 3 phần chính: một khối phân rã đa tầng kết hợp (Multi-scale hybrid decomposition – MHDecomp), một khối dự đoán cho dữ liệu có tính mùa vụ, một khối dự đoán có dữ liệu có tính xu hướng theo chu kỳ. Sau khi xử lý từng phần xong mô hình sẽ kết hợp kết quả của 2 quá trình lại để cho ra kết quả cuối cùng.

\subsubsection{MULTI-SCALE HYBRID DECOMPOSITION}
Khối phân rã đã tầng kết hợp được xây dựng dựa theo thuật toán phân rã của Haixu Wu là áp dụng trung bình trượt để làm mượt các biến động nhỏ và làm nổi bật các xu hướng dài hạn. Tuy nhiên thuật toán này có điểm yếu khi sử dụng tham số kernel nên ở mô hình MICN đã cải tiến bằng cách sử dụng nhiều kernel khác nhau kết hợp với việc sử dụng giá trị trung bình để xử lý. Với dữ liệu đầu vào là \( X \in \mathbb{R}^{I \times d} \), quá trình phân rã được xử lý như sau:
\[
X_t=\left( \text{AvgPool(Padding}(X))_{\text{kernel}_1}, \ldots, \text{AvgPool(Padding}(X))_{\text{kernel}_n} \right)
\]
Trong đó:
\begin{itemize}
    \item \( X_t \) là phần dữ liệu có xu hướng chu kỳ.
    \item \( X_s \) là phần dữ liệu có tính mùa vụ.
\end{itemize}

\subsubsection{TREND-CYCLICAL PREDICTION BLOCK}
Mô hình MICN sử dụng thuật toán hồi quy tuyến tính đơn giản để dự đoán cho dữ liệu có xu hướng theo chu kỳ. Cụ thể với dữ liệu có tính xu hướng theo chu kỳ được xử lý bằng cách:
\[Y{\substack{\text{regre}\\t}}{} = regression(X_t)\]
Trong đó \(Y_{\text{regre}t} \in \mathbb{R}^{O \times d}\) là kết quả dự đoán của phần này dựa trên thuật toán hồi quy tuyến tính.

\subsubsection{SEASONAL PREDICTION BLOCK}
Khối dự đoán cho dữ liệu có tính mùa vụ tập trung vào việc xử lý phần dữ liệu có tính mùa vụ phức tạp. Đầu tiên thực hiện quá trình Embedding chuỗi đầu vào $X_s$, thuật toán tiếp tục áp dụng “Multi-scale isometric convolution” (MIC) để lấy ra những đặc trưng cục bộ và tương quan toàn thể, và các nhánh với các tỷ lệ khác nhau, tức là các mạng con hoặc các tham số khác nhau được sử dụng để mô hình hóa các mẫu cơ bản khác nhau trong chuỗi thời gian. Mỗi nhánh có thể tập trung vào việc xác định mẫu ở một mức độ chi tiết khác nhau. Cuối cùng kết hợp các kết quả từ các nhánh khác nhau để hoàn thiện việc sử dụng thông tin toàn diện của chuỗi.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.8\linewidth}
    \centering
        \includegraphics[width=\linewidth]{images/MICN_SeasonalBlock.png}
    \caption{Khối dự đoán dữ liệu mùa vụ}
    \label{fig11}
    \end{minipage}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.8\linewidth}
    \centering
        \includegraphics[width=\linewidth]{images/MICN_LocalGlobalModule.png}
    \caption{Kiến trúc module cục bộ-toàn cầu}
    \label{fig11}
    \end{minipage}
\end{figure}
Các quá trình được tổng kết lại bằng các công thức sau:
\[X{\substack{\text{emb}\\s}}{} = {\text{Embedding}}(\text{Concat}(X_s, X_{\text{zero}}))\]
\[Y{\substack{\text{0}\\s}}{} = X{\substack{\text{emb}\\s}}{}\]
\[Y_{s,l} = \text{MIC}(Y_{s,l-1}), \quad l \in \{1, 2, \ldots, N\}\]
\[Y_s = \text{Truncate}(\text{Projection}(Y_{s,N}))\]

Trong đó:
\begin{itemize}
    \item \(X_{\text{zero}} \in \mathbb{R}^{O \times d}\) là các chỗ trống được điền bằng zero
    \item \(X_{\text{emb}}^s \in \mathbb{R}^{(I+O) \times D}\) là các dữ liệu đã được nhúng từ \(X_s\)
    \item \(Y_{s,l} \in \mathbb{R}^{(I+O) \times D}\) là kết quả dự đoán của lớp MIC thứ \(l\)
    \item \(Y_s\) là kết quả dự đoán cuối cùng của khối dự đoán cho dữ liệu có tính mùa vụ
\end{itemize}


\section{KẾT QUẢ}
\subsection{CÁC PHƯƠNG PHÁP ĐÁNH GIÁ}
MAPE (Mean Absolute Percentage Error): tính toán sự sai lệch trung bình theo tỷ lệ phần trăm giữa giá trị dự báo và giá trị thực tế. Tuy nhiên khi giá trị thực tế gần bằng 0, MAPE có thể dẫn đến phép chia cho 0 gây sai lệch trong kết quả đánh giá.
\[\text{MAPE} = \frac{1}{n} \sum_{t=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \times 100\%\]
Root Mean Squared Error (RMSE): là độ lệch chuẩn của các phần dư (sai số dự đoán). RMSE được tính bằng cách lấy căn bậc hai của trung bình của bình phương các sai số giữa giá trị dự đoán và giá trị thực tế. Tuy nhiên, RMSE có thể bị ảnh hưởng bởi các giá trị ngoại lai (outliers) trong dữ liệu. 
\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}\]

Mean Absolute Error (MAE): dùng để so sánh hiệu suất và đánh giá mức độ chính xác của các mô hình dự báo. MAE đo lường độ lỗi trung bình tuyệt đối giữa giá trị dự đoán và giá trị thực tế. MAE càng nhỏ thì mô hình dự đoán càng chính xác.
\[\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\]

Trong đó:
\begin{itemize}
    \item \(n\): số lượng mẫu
    \item \(y_i\): giá trị thực tế của mẫu thứ \(i\)
    \item \(\hat{y}_i\): giá trị dự đoán tương ứng của mô hình cho mẫu thứ \(i\)
\end{itemize}


\subsection{ĐÁNH GIÁ}

\begin{table}[h]
\centering
\caption{ĐÁNH GIÁ TRÊN BỘ DỮ LIỆU BIDV}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Ratio} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE (\%)} \\ \midrule
\multirow{3}{*}{Linear} & 7:3 & 6699.874 & 6089.079 & 32.65 \\
                        & 8:2 & 5730.287 & 5514.895 & 29.412 \\
                        & \textbf{9:1} & \textbf{4114.666} & \textbf{3621.554} & \textbf{19.495} \\ \midrule
\multirow{3}{*}{Holt-Winter} & 7:3 & 2958.645 & 2641.558 & 14.258 \\
                        & 8:2 & 2476.036 & 1614.404 & 7.586 \\
                        & \textbf{9:1} & \textbf{2647.877} & \textbf{1890.612} & \textbf{8.649} \\ \midrule
\multirow{3}{*}{ARIMA} & 7:3 & 2958.645 & 2641.558 & 14.258 \\
                        & 8:2 & 2476.036 & 1614.404 & 7.586 \\
                        & \textbf{9:1} & \textbf{2647.877} & \textbf{1890.612} & \textbf{8.649} \\ \midrule
\multirow{3}{*}{XGBoost} & 7:3 & 626.85 & 493.50 & 2.61 \\
                        & 8:2 & 425.11 & 305.03 & 1.57 \\
                        & \textbf{9:1} & \textbf{429.89} & \textbf{316.95} & \textbf{1.55} \\ \midrule
\multirow{3}{*}{\makecell{Linear \\ CalendarFourier, \\ DeterministicProcess}} & 7:3 & 626.85 & 493.50 & 2.61 \\
                        & 8:2 & 425.11 & 305.03 & 1.57 \\
                        & \textbf{9:1} & \textbf{429.89} & \textbf{316.95} & \textbf{1.55} \\
                        \midrule
\multirow{3}{*}{RNN} & 7:3 & 0.125 & 0.017 & 16.234 \\
                        & 8:2 & 0.136 & 0.014 & 16.99 \\
                        & \textbf{9:1} & \textbf{0.094} & \textbf{0.015} & \textbf{10.521} \\ \midrule
\multirow{3}{*}{GRU} & 7:3 & 799.344 & 565.852 & 1.3439 \\
                        & 8:2 & 778.219 & 550.714 & 1.233 \\
                        & \textbf{9:1} & \textbf{946.578} & \textbf{644.264} & \textbf{1.262} \\ \midrule
\multirow{3}{*}{LSTM} & 7:3 & 1140.567 & 943.25 & 2.215 \\
                        & 8:2 & 863.893 & 631.665 & 1.41 \\
                        & \textbf{9:1} & \textbf{991.127} & \textbf{723.549} & \textbf{1.418} \\ \midrule
\multirow{3}{*}{CNN-LSTM} & 7:3 & 831.436 & 594 & 1.4 \\
                        & 8:2 & 819.317 & 609.63 & 1.381 \\
                        & \textbf{9:1} & \textbf{963.615} & \textbf{673.057} & \textbf{1.32} \\
\multirow{3}{*}{MICN} & 7:3 & 4768.2027 & 3880.841 & 8.99 \\
                        & 8:2 & 5851.538 & 4515.326 & 9.73 \\
                        & \textbf{9:1} & \textbf{7823.79} & \textbf{6982.51} & \textbf{14.54} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{ĐÁNH GIÁ TRÊN BỘ DỮ LIỆU VCB}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Ratio} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE (\%)} \\ \midrule
\multirow{3}{*}{Linear} & 7:3 & 6699.874 & 6089.079 & 32.65 \\
                        & 8:2 & 5730.287 & 5514.895 & 29.412 \\
                        & \textbf{9:1} & \textbf{4114.666} & \textbf{3621.554} & \textbf{19.495} \\ \midrule
\multirow{3}{*}{Holt-Winter} & 7:3 & 2958.645 & 2641.558 & 14.258 \\
                        & 8:2 & 2476.036 & 1614.404 & 7.586 \\
                        & \textbf{9:1} & \textbf{2647.877} & \textbf{1890.612} & \textbf{8.649} \\ \midrule
\multirow{3}{*}{ARIMA} & 7:3 & 2958.645 & 2641.558 & 14.258 \\
                        & 8:2 & 2476.036 & 1614.404 & 7.586 \\
                        & \textbf{9:1} & \textbf{2647.877} & \textbf{1890.612} & \textbf{8.649} \\ \midrule
\multirow{3}{*}{XGBoost} & 7:3 & 626.85 & 493.50 & 2.61 \\
                        & 8:2 & 425.11 & 305.03 & 1.57 \\
                        & \textbf{9:1} & \textbf{429.89} & \textbf{316.95} & \textbf{1.55} \\ \midrule
\multirow{3}{*}{\makecell{Linear \\ CalendarFourier, \\ DeterministicProcess}} & 7:3 & 626.85 & 493.50 & 2.61 \\
                        & 8:2 & 425.11 & 305.03 & 1.57 \\
                        & \textbf{9:1} & \textbf{429.89} & \textbf{316.95} & \textbf{1.55} \\
                        \midrule
\multirow{3}{*}{RNN} & 7:3 & 0.125 & 0.017 & 16.234 \\
                        & 8:2 & 0.136 & 0.014 & 16.99 \\
                        & \textbf{9:1} & \textbf{0.094} & \textbf{0.015} & \textbf{10.521} \\ \midrule
\multirow{3}{*}{GRU} & 7:3 & 1505.262 & 1187.511 & 1.42 \\
                        & 8:2 & 1292.515 & 964.056 & 1.076 \\
                        & \textbf{9:1} & \textbf{879.522} & \textbf{684.756} & \textbf{0.735} \\ \midrule
\multirow{3}{*}{LSTM} & 7:3 & 1626.77 & 1346.2 & 1.636 \\
                        & 8:2 & 1170.171 & 869.401 & 0.969 \\
                        & \textbf{9:1} & \textbf{848.022} & \textbf{649.84} & \textbf{0.696} \\ \midrule
\multirow{3}{*}{CNN-LSTM} & 7:3 & 2344.712 & 2083.6 & 2.462 \\
                        & 8:2 & 1600.824 & 1321.838 & 1.472 \\
                        & \textbf{9:1} & \textbf{1029.217} & \textbf{763.943} & \textbf{0.817}\\ \midrule
\multirow{3}{*}{MICN} & 7:3 & 7171.965 & 6384.823 & 7.61 \\
                        & 8:2 & 13536.513 & 13263.265 & 15.24 \\
                        & \textbf{9:1} & \textbf{14089.16} & \textbf{13939.425} & \textbf{15.53} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{ĐÁNH GIÁ TRÊN BỘ DỮ LIỆU MBB}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Ratio} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE (\%)} \\ \midrule
\multirow{3}{*}{Linear} & 7:3 & 6699.874 & 6089.079 & 32.65 \\
                        & 8:2 & 5730.287 & 5514.895 & 29.412 \\
                        & \textbf{9:1} & \textbf{4114.666} & \textbf{3621.554} & \textbf{19.495} \\ \midrule
\multirow{3}{*}{Holt-Winter} & 7:3 & 2958.645 & 2641.558 & 14.258 \\
                        & 8:2 & 2476.036 & 1614.404 & 7.586 \\
                        & \textbf{9:1} & \textbf{2647.877} & \textbf{1890.612} & \textbf{8.649} \\ \midrule
\multirow{3}{*}{ARIMA} & 7:3 & 2958.645 & 2641.558 & 14.258 \\
                        & 8:2 & 2476.036 & 1614.404 & 7.586 \\
                        & \textbf{9:1} & \textbf{2647.877} & \textbf{1890.612} & \textbf{8.649} \\ \midrule
\multirow{3}{*}{XGBoost} & 7:3 & 626.85 & 493.50 & 2.61 \\
                        & 8:2 & 425.11 & 305.03 & 1.57 \\
                        & \textbf{9:1} & \textbf{429.89} & \textbf{316.95} & \textbf{1.55} \\ \midrule
\multirow{3}{*}{\makecell{Linear \\ CalendarFourier, \\ DeterministicProcess}} & 7:3 & 626.85 & 493.50 & 2.61 \\
                        & 8:2 & 425.11 & 305.03 & 1.57 \\
                        & \textbf{9:1} & \textbf{429.89} & \textbf{316.95} & \textbf{1.55} \\
                        \midrule
\multirow{3}{*}{RNN} & 7:3 & 0.125 & 0.017 & 16.234 \\
                        & 8:2 & 0.136 & 0.014 & 16.99 \\
                        & \textbf{9:1} & \textbf{0.094} & \textbf{0.015} & \textbf{10.521} \\ \midrule
\multirow{3}{*}{GRU} & 7:3 & 0.125 & 0.017 & 16.234 \\
                        & 8:2 & 0.136 & 0.014 & 16.99 \\
                        & \textbf{9:1} & \textbf{0.094} & \textbf{0.015} & \textbf{10.521} \\ \midrule
\multirow{3}{*}{LSTM} & 7:3 & 364.172 & 262.703 & 1.48 \\
                        & 8:2 & 309.524 & 219.665 & 1.075 \\
                        & \textbf{9:1} & \textbf{424.367} & \textbf{329.628} & \textbf{1.421} \\ \midrule
\multirow{3}{*}{CNN-LSTM} & 7:3 & 379.135 & 293.236 & 1.633 \\
                        & 8:2 & 342.78 & 258.295 & 1.276 \\
                        & \textbf{9:1} & \textbf{427.179} & \textbf{339.616} & \textbf{1.475} \\
\multirow{3}{*}{MICN} & 7:3 & 1455.624 & 1397.894 & 8.16 \\
                        & 8:2 & 700.222 & 619.938 & 3.57 \\
                        & \textbf{9:1} & \textbf{1057.847} & \textbf{ 952.305} & \textbf{4.84} \\ \bottomrule
\end{tabular}
\end{table}

\subsection{VISUALIZATION}

\subsubsection{ĐÁNH GIÁ TRÊN BỘ DỮ LIỆU BIDV}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR/LR_BIDV_82.png}
    \captionof{figure}{Linear Regression 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR/LR_BIDV_91.png}
    \captionof{figure}{Linear Regression 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Holt Winter/HW_BIDV_82.png}
    \captionof{figure}{Holt-Winter 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Holt Winter/HW_BIDV_91.png}
    \captionof{figure}{Holt-Winter 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ARIMA/ARIMA_BIDV_82.png}
    \captionof{figure}{ARIMA 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ARIMA/ARIMA_BIDV_91.png}
    \captionof{figure}{ARIMA 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/XGBoost/XGBoost_BIDV_82.png}
    \captionof{figure}{XGBoost 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/XGBoost/XGBoost_BIDV_91.png}
    \captionof{figure}{XGBoost 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR CalendarFourier, DeterministicProcess/LRCD_BIDV_91.png}
    \captionof{figure}{Linear Regression áp dụng CalendarFourier, DeterministicProcess 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR CalendarFourier, DeterministicProcess/LRCD_BIDV_82.png}
    \captionof{figure}{Linear Regression áp dụng CalendarFourier, DeterministicProcess 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/GRU/GRU_BIDV_82.png}
    \captionof{figure}{GRU with 8:2 ratio}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/GRU/GRU_BIDV_91.png}
    \captionof{figure}{GRU 9:1 }
    \label{fig:image2}
\end{minipage}


\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/RNN/RNN_BIDV_82.png}
    \captionof{figure}{RNN 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/RNN/RNN_BIDV_91.png}
    \captionof{figure}{RNN 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_BIDV_82.png}
    \captionof{figure}{LSTM 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_BIDV_91.png}
    \captionof{figure}{LSTM 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_BIDV_82.png}
    \captionof{figure}{CNN-LSTM 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_BIDV_91.png}
    \captionof{figure}{CNN-LSTM 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/MICN/MICN_BIDV_82.png}
    \captionof{figure}{MICN 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/MICN/MICN_BIDV_91.png}
    \captionof{figure}{MICN 9:1 }
    \label{fig:image2}
\end{minipage}





\subsubsection{VCB DATASET VISUALIZATION}
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR/LR_VCB_82.png}
    \captionof{figure}{Linear Regression 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR/LR_VCB_91.png}
    \captionof{figure}{Linear Regression 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Holt Winter/HW_VCB_82.png}
    \captionof{figure}{Holt-Winter 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Holt Winter/HW_VCB_91.png}
    \captionof{figure}{Holt-Winter 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ARIMA/ARIMA_VCB_82.png}
    \captionof{figure}{ARIMA 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ARIMA/ARIMA_VCB_91.png}
    \captionof{figure}{ARIMA 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/XGBoost/XGBoost_VCB_82.png}
    \captionof{figure}{XGBoost 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/XGBoost/XGBoost_VCB_91.png}
    \captionof{figure}{XGBoost 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR CalendarFourier, DeterministicProcess/LRCD_VCB_82.png}
    \captionof{figure}{Linear Regression áp dụng CalendarFourier, DeterministicProcess 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR CalendarFourier, DeterministicProcess/LRCD_VCB_91.png}}
    \captionof{figure}{Linear Regression áp dụng CalendarFourier, DeterministicProcess 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/GRU/GRU_VCB_82.png}
    \captionof{figure}{GRU 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/GRU/GRU_VCB_91.png}
    \captionof{figure}{GRU 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/RNN/RNN_VCB_82.png}
    \captionof{figure}{RNN 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/RNN/RNN_VCB_91.png}
    \captionof{figure}{RNN 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LSTM/LSTM_VCB_82.png}
    \captionof{figure}{LSTM 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LSTM/LSTM_VCB_91.png}
    \captionof{figure}{LSTM 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_VCB_82.png}
    \captionof{figure}{CNN-LSTM 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_VCB_91.png}
    \captionof{figure}{CNN-LSTM 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/MICN/MICN_VCB_82.png}
    \captionof{figure}{MICN 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/MICN/MICN_VCB_91.png}
    \captionof{figure}{MICN 9:1 }
    \label{fig:image2}
\end{minipage}





\subsubsection{MBB DATASET VISUALIZATION}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR/LR_MBB_82.png}
    \captionof{figure}{Linear Regression 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR/LR_MBB_91.png}
    \captionof{figure}{Linear Regression 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Holt Winter/HW_MBB_82.png}
    \captionof{figure}{Holt-Winter 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Holt Winter/HW_MBB_91.png}
    \captionof{figure}{Holt-Winter 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ARIMA/ARIMA_MBB_82.png}
    \captionof{figure}{ARIMA 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ARIMA/ARIMA_MBB_91.png}
    \captionof{figure}{ARIMA 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/XGBoost/XGBoost_MBB_82.png}
    \captionof{figure}{XGBoost 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/XGBoost/XGBoost_MBB_91.png}
    \captionof{figure}{XGBoost 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR CalendarFourier, DeterministicProcess/LRCD_MBB_82.png}
    \captionof{figure}{Linear Regression áp dụng CalendarFourier, DeterministicProcess 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LR CalendarFourier, DeterministicProcess/LRCD_MBB_91.png}}
    \captionof{figure}{Linear Regression áp dụng CalendarFourier, DeterministicProcess 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/GRU/GRU_MBB_82.png}
    \captionof{figure}{GRU 8:2 }
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/GRU/GRU_MBB_91.png}
    \captionof{figure}{GRU 9:1 }
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/RNN/RNN_MBB_82.png}
    \captionof{figure}{RNN 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/RNN/RNN_MBB_91.png}
    \captionof{figure}{RNN 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LSTM/LSTM_MBB_82.png}
    \captionof{figure}{LSTM 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/LSTM/LSTM_MBB_91.png}
    \captionof{figure}{LSTM 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_MBB_82.png}
    \captionof{figure}{CNN-LSTM 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/CNN-LSTM/CNNLSTM_MBB_91.png}
    \captionof{figure}{CNN-LSTM 9:1}
    \label{fig:image2}
\end{minipage}

\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/MICN/MICN_MBB_82.png}
    \captionof{figure}{MICN 8:2}
    \label{fig:image1}
\end{minipage}
\hfill
\begin{minipage}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/MICN/MICN_MBB_91.png}
    \captionof{figure}{MICN 9:1}
    \label{fig:image2}
\end{minipage}

\printbibliography

\end{document}
