{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: batch,seq_len,channels\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class series_decomp_multi(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp_multi, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.moving_avg = [moving_avg(kernel, stride=1) for kernel in kernel_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = []\n",
    "        res = []\n",
    "        for func in self.moving_avg:\n",
    "            moving_avg = func(x)\n",
    "            moving_mean.append(moving_avg)\n",
    "            sea = x - moving_avg\n",
    "            res.append(sea)\n",
    "\n",
    "        sea = sum(res) / len(res)\n",
    "        moving_mean = sum(moving_mean) / len(moving_mean)\n",
    "        return sea, moving_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        self.initialize_weight(self.layer1)\n",
    "        self.initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_weight(self, x):\n",
    "        nn.init.xavier_uniform_(x.weight)\n",
    "        if x.bias is not None:\n",
    "            nn.init.constant_(x.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIC(nn.Module):\n",
    "    \"\"\"\n",
    "    MIC layer to extract local and global features\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_size=512, n_heads=8, dropout=0.05, decomp_kernel=[32], conv_kernel=[24], isometric_kernel=[18, 6], device='cuda'):\n",
    "        super(MIC, self).__init__()\n",
    "        self.src_mask = None\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.isometric_kernel = isometric_kernel\n",
    "        self.device = device\n",
    "        \n",
    "        # isometric convolution\n",
    "        self.isometric_conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                                   kernel_size=i,padding=0,stride=1)\n",
    "                                        for i in isometric_kernel])\n",
    "\n",
    "        # downsampling convolution: padding=i//2, stride=i\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                             kernel_size=i,padding=i//2,stride=i)\n",
    "                                  for i in conv_kernel])\n",
    "\n",
    "        # upsampling convolution\n",
    "        self.conv_trans = nn.ModuleList([nn.ConvTranspose1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                                            kernel_size=i,padding=0,stride=i)\n",
    "                                        for i in conv_kernel])\n",
    "\n",
    "        self.decomp = nn.ModuleList([series_decomp(k) for k in decomp_kernel])\n",
    "        self.merge = torch.nn.Conv2d(in_channels=feature_size, out_channels=feature_size, kernel_size=(len(self.conv_kernel), 1))\n",
    "\n",
    "        self.fnn = FeedForwardNetwork(feature_size, feature_size*4, dropout)\n",
    "        self.fnn_norm = torch.nn.LayerNorm(feature_size)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(feature_size)\n",
    "        self.act = torch.nn.Tanh()\n",
    "        self.drop = torch.nn.Dropout(0.05)\n",
    "\n",
    "    def conv_trans_conv(self, input, conv1d, conv1d_trans, isometric):\n",
    "        batch, seq_len, channel = input.shape\n",
    "        x = input.permute(0, 2, 1)\n",
    "\n",
    "        # downsampling convolution\n",
    "        x1 = self.drop(self.act(conv1d(x)))\n",
    "        x = x1\n",
    "\n",
    "        # isometric convolution \n",
    "        zeros = torch.zeros((x.shape[0], x.shape[1], x.shape[2]-1), device=self.device)\n",
    "        x = torch.cat((zeros, x), dim=-1)\n",
    "        x = self.drop(self.act(isometric(x)))\n",
    "        x = self.norm((x+x1).permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        # upsampling convolution\n",
    "        x = self.drop(self.act(conv1d_trans(x)))\n",
    "        x = x[:, :, :seq_len]   # truncate\n",
    "\n",
    "        x = self.norm(x.permute(0, 2, 1) + input)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        # multi-scale\n",
    "        multi = []  \n",
    "        for i in range(len(self.conv_kernel)):\n",
    "            src_out, trend1 = self.decomp[i](src)\n",
    "            src_out = self.conv_trans_conv(src_out, self.conv[i], self.conv_trans[i], self.isometric_conv[i])\n",
    "            multi.append(src_out)  \n",
    "\n",
    "        # merge\n",
    "        mg = torch.tensor([], device = self.device)\n",
    "        for i in range(len(self.conv_kernel)):\n",
    "            mg = torch.cat((mg, multi[i].unsqueeze(1)), dim=1)\n",
    "        mg = self.merge(mg.permute(0,3,1,2)).squeeze(-2).permute(0,2,1)\n",
    "        \n",
    "        return self.fnn_norm(mg + self.fnn(mg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seasonal_Prediction(nn.Module):\n",
    "    def __init__(self, embedding_size=512, n_heads=8, dropout=0.05, d_layers=1, decomp_kernel=[32], c_out=1,\n",
    "                conv_kernel=[2, 4], isometric_kernel=[18, 6], device='cuda'):\n",
    "        super(Seasonal_Prediction, self).__init__()\n",
    "\n",
    "        self.mic = nn.ModuleList([MIC(feature_size=embedding_size, n_heads=n_heads,\n",
    "                                                   decomp_kernel=decomp_kernel,conv_kernel=conv_kernel, isometric_kernel=isometric_kernel, device=device)\n",
    "                                      for i in range(d_layers)])\n",
    "\n",
    "        self.projection = nn.Linear(embedding_size, c_out)\n",
    "\n",
    "    def forward(self, dec):\n",
    "        for mic_layer in self.mic:\n",
    "            dec = mic_layer(dec)\n",
    "        return self.projection(dec)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
