{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy 1 lần \n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "# !pip install torchvision\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class my_Layernorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class series_decomp_multi(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple Series decomposition block from FEDformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp_multi, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.series_decomp = [series_decomp(kernel) for kernel in kernel_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = []\n",
    "        res = []\n",
    "        for func in self.series_decomp:\n",
    "            sea, moving_avg = func(x)\n",
    "            moving_mean.append(moving_avg)\n",
    "            res.append(sea)\n",
    "\n",
    "        sea = sum(res) / len(res)\n",
    "        moving_mean = sum(moving_mean) / len(moving_mean)\n",
    "        return sea, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer decoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,\n",
    "                 moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.decomp3 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=3, stride=1, padding=1,\n",
    "                                    padding_mode='circular', bias=False)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x, trend1 = self.decomp1(x)\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "        x, trend2 = self.decomp2(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        x, trend3 = self.decomp3(x + y)\n",
    "\n",
    "        residual_trend = trend1 + trend2 + trend3\n",
    "        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x, residual_trend\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n",
    "        for layer in self.layers:\n",
    "            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "            trend = trend + residual_trend\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x, trend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3, 'D': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(\n",
    "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DataEmbedding_inverted(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding_inverted, self).__init__()\n",
    "        self.value_embedding = nn.Linear(c_in, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # x: [Batch Variate Time]\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1))\n",
    "        # x: [Batch Variate d_model]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DataEmbedding_wo_pos(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding_wo_pos, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, patch_len, stride, padding, dropout):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch_layer = nn.ReplicationPad1d((0, padding))\n",
    "\n",
    "        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space\n",
    "        self.value_embedding = nn.Linear(patch_len, d_model, bias=False)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do patching\n",
    "        n_vars = x.shape[1]\n",
    "        x = self.padding_patch_layer(x)\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # Input encoding\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        return self.dropout(x), n_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "from einops import rearrange, reduce, repeat\n",
    "import math, random\n",
    "from scipy.fftpack import next_fast_len\n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def transform(self, x):\n",
    "        return self.jitter(self.shift(self.scale(x)))\n",
    "\n",
    "    def jitter(self, x):\n",
    "        return x + (torch.randn(x.shape).to(x.device) * self.sigma)\n",
    "\n",
    "    def scale(self, x):\n",
    "        return x * (torch.randn(x.size(-1)).to(x.device) * self.sigma + 1)\n",
    "\n",
    "    def shift(self, x):\n",
    "        return x + (torch.randn(x.size(-1)).to(x.device) * self.sigma)\n",
    "\n",
    "\n",
    "def conv1d_fft(f, g, dim=-1):\n",
    "    N = f.size(dim)\n",
    "    M = g.size(dim)\n",
    "\n",
    "    fast_len = next_fast_len(N + M - 1)\n",
    "\n",
    "    F_f = fft.rfft(f, fast_len, dim=dim)\n",
    "    F_g = fft.rfft(g, fast_len, dim=dim)\n",
    "\n",
    "    F_fg = F_f * F_g.conj()\n",
    "    out = fft.irfft(F_fg, fast_len, dim=dim)\n",
    "    out = out.roll((-1,), dims=(dim,))\n",
    "    idx = torch.as_tensor(range(fast_len - N, fast_len)).to(out.device)\n",
    "    out = out.index_select(dim, idx)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class ExponentialSmoothing(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, nhead, dropout=0.1, aux=False):\n",
    "        super().__init__()\n",
    "        self._smoothing_weight = nn.Parameter(torch.randn(nhead, 1))\n",
    "        self.v0 = nn.Parameter(torch.randn(1, 1, nhead, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if aux:\n",
    "            self.aux_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, values, aux_values=None):\n",
    "        b, t, h, d = values.shape\n",
    "\n",
    "        init_weight, weight = self.get_exponential_weight(t)\n",
    "        output = conv1d_fft(self.dropout(values), weight, dim=1)\n",
    "        output = init_weight * self.v0 + output\n",
    "\n",
    "        if aux_values is not None:\n",
    "            aux_weight = weight / (1 - self.weight) * self.weight\n",
    "            aux_output = conv1d_fft(self.aux_dropout(aux_values), aux_weight)\n",
    "            output = output + aux_output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_exponential_weight(self, T):\n",
    "        # Generate array [0, 1, ..., T-1]\n",
    "        powers = torch.arange(T, dtype=torch.float, device=self.weight.device)\n",
    "\n",
    "        # (1 - \\alpha) * \\alpha^t, for all t = T-1, T-2, ..., 0]\n",
    "        weight = (1 - self.weight) * (self.weight ** torch.flip(powers, dims=(0,)))\n",
    "\n",
    "        # \\alpha^t for all t = 1, 2, ..., T\n",
    "        init_weight = self.weight ** (powers + 1)\n",
    "\n",
    "        return rearrange(init_weight, 'h t -> 1 t h 1'), \\\n",
    "               rearrange(weight, 'h t -> 1 t h 1')\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return torch.sigmoid(self._smoothing_weight)\n",
    "\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout=0.1, activation='sigmoid'):\n",
    "        # Implementation of Feedforward model\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, bias=False)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, bias=False)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = getattr(F, activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class GrowthLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, d_head=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head or (d_model // nhead)\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.z0 = nn.Parameter(torch.randn(self.nhead, self.d_head))\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_head * self.nhead)\n",
    "        self.es = ExponentialSmoothing(self.d_head, self.nhead, dropout=dropout)\n",
    "        self.out_proj = nn.Linear(self.d_head * self.nhead, self.d_model)\n",
    "\n",
    "        assert self.d_head * self.nhead == self.d_model, \"d_model must be divisible by nhead\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: shape: (batch, seq_len, dim)\n",
    "        :return: shape: (batch, seq_len, dim)\n",
    "        \"\"\"\n",
    "        b, t, d = inputs.shape\n",
    "        values = self.in_proj(inputs).view(b, t, self.nhead, -1)\n",
    "        values = torch.cat([repeat(self.z0, 'h d -> b 1 h d', b=b), values], dim=1)\n",
    "        values = values[:, 1:] - values[:, :-1]\n",
    "        out = self.es(values)\n",
    "        out = torch.cat([repeat(self.es.v0, '1 1 h d -> b 1 h d', b=b), out], dim=1)\n",
    "        out = rearrange(out, 'b t h d -> b t (h d)')\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, pred_len, k=None, low_freq=1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pred_len = pred_len\n",
    "        self.k = k\n",
    "        self.low_freq = low_freq\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (b, t, d)\"\"\"\n",
    "        b, t, d = x.shape\n",
    "        x_freq = fft.rfft(x, dim=1)\n",
    "\n",
    "        if t % 2 == 0:\n",
    "            x_freq = x_freq[:, self.low_freq:-1]\n",
    "            f = fft.rfftfreq(t)[self.low_freq:-1]\n",
    "        else:\n",
    "            x_freq = x_freq[:, self.low_freq:]\n",
    "            f = fft.rfftfreq(t)[self.low_freq:]\n",
    "\n",
    "        x_freq, index_tuple = self.topk_freq(x_freq)\n",
    "        f = repeat(f, 'f -> b f d', b=x_freq.size(0), d=x_freq.size(2))\n",
    "        f = rearrange(f[index_tuple], 'b f d -> b f () d').to(x_freq.device)\n",
    "\n",
    "        return self.extrapolate(x_freq, f, t)\n",
    "\n",
    "    def extrapolate(self, x_freq, f, t):\n",
    "        x_freq = torch.cat([x_freq, x_freq.conj()], dim=1)\n",
    "        f = torch.cat([f, -f], dim=1)\n",
    "        t_val = rearrange(torch.arange(t + self.pred_len, dtype=torch.float),\n",
    "                          't -> () () t ()').to(x_freq.device)\n",
    "\n",
    "        amp = rearrange(x_freq.abs() / t, 'b f d -> b f () d')\n",
    "        phase = rearrange(x_freq.angle(), 'b f d -> b f () d')\n",
    "\n",
    "        x_time = amp * torch.cos(2 * math.pi * f * t_val + phase)\n",
    "\n",
    "        return reduce(x_time, 'b f t d -> b t d', 'sum')\n",
    "\n",
    "    def topk_freq(self, x_freq):\n",
    "        values, indices = torch.topk(x_freq.abs(), self.k, dim=1, largest=True, sorted=True)\n",
    "        mesh_a, mesh_b = torch.meshgrid(torch.arange(x_freq.size(0)), torch.arange(x_freq.size(2)))\n",
    "        index_tuple = (mesh_a.unsqueeze(1), indices, mesh_b.unsqueeze(1))\n",
    "        x_freq = x_freq[index_tuple]\n",
    "\n",
    "        return x_freq, index_tuple\n",
    "\n",
    "\n",
    "class LevelLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, c_out, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.c_out = c_out\n",
    "\n",
    "        self.es = ExponentialSmoothing(1, self.c_out, dropout=dropout, aux=True)\n",
    "        self.growth_pred = nn.Linear(self.d_model, self.c_out)\n",
    "        self.season_pred = nn.Linear(self.d_model, self.c_out)\n",
    "\n",
    "    def forward(self, level, growth, season):\n",
    "        b, t, _ = level.shape\n",
    "        growth = self.growth_pred(growth).view(b, t, self.c_out, 1)\n",
    "        season = self.season_pred(season).view(b, t, self.c_out, 1)\n",
    "        growth = growth.view(b, t, self.c_out, 1)\n",
    "        season = season.view(b, t, self.c_out, 1)\n",
    "        level = level.view(b, t, self.c_out, 1)\n",
    "        out = self.es(level - season, aux_values=growth)\n",
    "        out = rearrange(out, 'b t h d -> b t (h d)')\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, c_out, seq_len, pred_len, k, dim_feedforward=None, dropout=0.1,\n",
    "                 activation='sigmoid', layer_norm_eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.c_out = c_out\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        dim_feedforward = dim_feedforward or 4 * d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.growth_layer = GrowthLayer(d_model, nhead, dropout=dropout)\n",
    "        self.seasonal_layer = FourierLayer(d_model, pred_len, k=k)\n",
    "        self.level_layer = LevelLayer(d_model, c_out, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.ff = Feedforward(d_model, dim_feedforward, dropout=dropout, activation=activation)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, res, level, attn_mask=None):\n",
    "        season = self._season_block(res)\n",
    "        res = res - season[:, :-self.pred_len]\n",
    "        growth = self._growth_block(res)\n",
    "        res = self.norm1(res - growth[:, 1:])\n",
    "        res = self.norm2(res + self.ff(res))\n",
    "\n",
    "        level = self.level_layer(level, growth[:, :-1], season[:, :-self.pred_len])\n",
    "        return res, level, growth, season\n",
    "\n",
    "    def _growth_block(self, x):\n",
    "        x = self.growth_layer(x)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _season_block(self, x):\n",
    "        x = self.seasonal_layer(x)\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, res, level, attn_mask=None):\n",
    "        growths = []\n",
    "        seasons = []\n",
    "        for layer in self.layers:\n",
    "            res, level, growth, season = layer(res, level, attn_mask=None)\n",
    "            growths.append(growth)\n",
    "            seasons.append(season)\n",
    "\n",
    "        return level, growths, seasons\n",
    "\n",
    "\n",
    "class DampingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, pred_len, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.nhead = nhead\n",
    "        self._damping_factor = nn.Parameter(torch.randn(1, nhead))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = repeat(x, 'b 1 d -> b t d', t=self.pred_len)\n",
    "        b, t, d = x.shape\n",
    "\n",
    "        powers = torch.arange(self.pred_len).to(self._damping_factor.device) + 1\n",
    "        powers = powers.view(self.pred_len, 1)\n",
    "        damping_factors = self.damping_factor ** powers\n",
    "        damping_factors = damping_factors.cumsum(dim=0)\n",
    "        x = x.view(b, t, self.nhead, -1)\n",
    "        x = self.dropout(x) * damping_factors.unsqueeze(-1)\n",
    "        return x.view(b, t, d)\n",
    "\n",
    "    @property\n",
    "    def damping_factor(self):\n",
    "        return torch.sigmoid(self._damping_factor)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, c_out, pred_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.c_out = c_out\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.growth_damping = DampingLayer(pred_len, nhead, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, growth, season):\n",
    "        growth_horizon = self.growth_damping(growth[:, -1:])\n",
    "        growth_horizon = self.dropout1(growth_horizon)\n",
    "\n",
    "        seasonal_horizon = season[:, -self.pred_len:]\n",
    "        return growth_horizon, seasonal_horizon\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.d_model = layers[0].d_model\n",
    "        self.c_out = layers[0].c_out\n",
    "        self.pred_len = layers[0].pred_len\n",
    "        self.nhead = layers[0].nhead\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.pred = nn.Linear(self.d_model, self.c_out)\n",
    "\n",
    "    def forward(self, growths, seasons):\n",
    "        growth_repr = []\n",
    "        season_repr = []\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            growth_horizon, season_horizon = layer(growths[idx], seasons[idx])\n",
    "            growth_repr.append(growth_horizon)\n",
    "            season_repr.append(season_horizon)\n",
    "        growth_repr = sum(growth_repr)\n",
    "        season_repr = sum(season_repr)\n",
    "        return self.pred(growth_repr), self.pred(season_repr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from Model.MICN.layers.Embed import DataEmbedding\n",
    "# from Model.MICN.layers.Autoformer_EncDec import series_decomp, series_decomp_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giả sử các lớp DataEmbedding, series_decomp, và series_decomp_multi đã được định nghĩa trong layers.Embed và layers.Autoformer_EncDec.\n",
    "\n",
    "\n",
    "class MIC(nn.Module):\n",
    "    \"\"\"\n",
    "    Mô hình MIC đơn giản với một lớp Linear.\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(MIC, self).__init__()\n",
    "        self.layer = nn.Linear(configs.d_model, configs.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeasonalPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    Khối dự báo mùa vụ với một lớp Linear.\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(SeasonalPrediction, self).__init__()\n",
    "        self.layer = nn.Linear(configs.d_model, configs.pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Mô hình chính bao gồm các khối embedding, encoder và decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=configs.d_model, nhead=configs.n_heads, dropout=configs.dropout),\n",
    "            num_layers=configs.d_layers)\n",
    "        self.decoder = nn.Linear(configs.d_model, 1)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        enc_out = self.embedding(x_enc, x_mark_enc)\n",
    "        enc_out = self.encoder(enc_out)\n",
    "        dec_out = self.decoder(enc_out[:, -1, :])\n",
    "        return dec_out.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecast(true_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Hàm tính toán các chỉ số đánh giá cho dự báo.\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(np.mean((true_values - predicted_values) ** 2))\n",
    "    mape = np.mean(np.abs((true_values - predicted_values) / true_values)) * 100\n",
    "    mae = np.mean(np.abs(true_values - predicted_values))\n",
    "    return rmse, mape, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def export_forecast_by_datasets(datasets, index_col, attribute, train_ratios, prediction_lengths, name):\n",
    "    \"\"\"\n",
    "    Hàm xuất dự báo cho các tập dữ liệu khác nhau.\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        i=0\n",
    "        bank_name = name[i]\n",
    "        i+=1\n",
    "        # Load và tiền xử lý dữ liệu\n",
    "        data = pd.read_csv(dataset, index_col=index_col)\n",
    "        \n",
    "        data.index = pd.to_datetime(data.index, format='%m/%d/%Y')\n",
    "        data[attribute] = data[attribute].str.replace(',', '').astype(float)\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        for train_ratio in train_ratios:\n",
    "            # Chia dữ liệu thành tập train và test\n",
    "            target = data[attribute].values\n",
    "            train_size = int(len(target) * train_ratio)\n",
    "\n",
    "            # Chuẩn hóa dữ liệu\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaled_data = scaler.fit_transform(target.reshape(-1, 1))\n",
    "            train_data = scaled_data[0:train_size, :]\n",
    "\n",
    "            # Chuẩn bị dữ liệu train\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "            for i in range(1, len(train_data)):\n",
    "                x_train.append(train_data[i-1:i, 0])\n",
    "                y_train.append(train_data[i, 0])\n",
    "            x_train = np.array(x_train)\n",
    "            y_train = np.array(y_train)\n",
    "            x_train, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "            x_train = x_train.unsqueeze(2)\n",
    "\n",
    "            # Chuẩn bị dữ liệu test\n",
    "            test_data = scaled_data[train_size-1:, :]\n",
    "            x_test = []\n",
    "            y_test = target[train_size:]\n",
    "            for i in range(1, len(test_data)):\n",
    "                x_test.append(test_data[i-1:i, 0])\n",
    "            x_test = np.array(x_test)\n",
    "            x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "            x_test = x_test.unsqueeze(2)\n",
    "\n",
    "            # Định nghĩa các cấu hình của mô hình\n",
    "            configs = type('', (), {})()\n",
    "            configs.task_name = 'long_term_forecast'\n",
    "            configs.enc_in = 1\n",
    "            configs.d_model = 128\n",
    "            configs.embed = 'timeF'\n",
    "            configs.freq = 'D'\n",
    "            configs.dropout = 0.05\n",
    "            configs.n_heads = 8\n",
    "            configs.d_layers = 3\n",
    "\n",
    "            # Khởi tạo mô hình và chuyển nó sang thiết bị phù hợp\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model = Model(configs)\n",
    "            model.to(device)\n",
    "\n",
    "            # Train model\n",
    "            model.train()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "            criterion = nn.MSELoss()\n",
    "            for epoch in range(100):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_train.to(device), None, x_test.to(device), None)\n",
    "                loss = criterion(output, y_train.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Chuyển mô hình sang chế độ đánh giá\n",
    "            model.eval()\n",
    "\n",
    "            # # Các bước chuẩn bị biểu đồ\n",
    "            # plt.figure(dpi=120)\n",
    "            # plt.xlabel(index_col)\n",
    "            # plt.ylabel(attribute)\n",
    "            # # plt.plot(data.index[:train_size], target[:train_size], label='Training Data')\n",
    "            # plt.plot(data.index[train_size:], target[train_size:], label='Actual Data')\n",
    "\n",
    "            # last_date = data.index[-1]\n",
    "\n",
    "            # # Tạo và vẽ dự báo cho các độ dài khác nhau\n",
    "            # for pred_len in prediction_lengths:\n",
    "            #     configs.pred_len = pred_len\n",
    "\n",
    "            #     with torch.no_grad():\n",
    "            #         predictions = model(x_test.to(device), None, torch.zeros(1, pred_len, 1, dtype=torch.float32).to(device), None)\n",
    "            #         predictions = scaler.inverse_transform(predictions.cpu().numpy().reshape(-1, 1))\n",
    "\n",
    "            #     forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=pred_len)\n",
    "            #     plt.plot(forecast_dates, predictions[:pred_len], label=f'Next {pred_len} Days Forecast')\n",
    "\n",
    "            # # Tính toán và in các chỉ số đánh giá hiệu suất\n",
    "            # rmse, mape, mae = evaluate_forecast(y_test[:pred_len], predictions[-pred_len:].flatten())\n",
    "            # print(f'Dataset: {dataset}, Train Ratio: {train_ratio}, Prediction Length: {pred_len}')\n",
    "            # print(f'RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, MAE: {mae:.4f}\\n')\n",
    "\n",
    "            # plt.legend(loc='upper left')\n",
    "            # plt.title(f'MICN - {dataset.split(\".\")[0]} cpyto price (Ratio {int(train_ratio*10)}:{int((10 - train_ratio*10))})')\n",
    "            # plt.show()\n",
    "            # Vẽ biểu đồ\n",
    "            plt.figure(dpi=120)\n",
    "            plt.xlabel(index_col)\n",
    "            plt.ylabel(attribute)\n",
    "            last_date = data.index[-1]\n",
    "            color_plot = ['C3','C4','C5']\n",
    "            for j, pred_len in enumerate(prediction_lengths):\n",
    "                configs.pred_len = pred_len\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(x_test.to(device), None, torch.zeros(1, pred_len, 1, dtype=torch.float32).to(device), None)\n",
    "                    predictions = scaler.inverse_transform(predictions.cpu().numpy().reshape(-1, 1))\n",
    "\n",
    "                forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=pred_len)\n",
    "                plt.plot(forecast_dates, predictions[:pred_len], label=f'Next {pred_len} Days Forecast', color = color_plot[j%len(color_plot)])\n",
    "\n",
    "                \n",
    "                # rmse, mape, mae = evaluate_forecast(y_test[:pred_len], predictions[-pred_len:].flatten())\n",
    "                # print(f'Dataset: {dataset}, Train Ratio: {train_ratio}, Prediction Length: {pred_len}')\n",
    "                # print(f'RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, MAE: {mae:.4f}\\n')\n",
    "            train = target[:train_size]\n",
    "            valid = target[train_size:]\n",
    "            valid_df = pd.DataFrame(valid, index=data.index[train_size:])\n",
    "            valid_df['Price'] = valid_df[0]\n",
    "            valid_df['Predictions'] = predictions[:len(valid)].squeeze()\n",
    "            \n",
    "            plt.plot(data.index[:train_size], train, label='Training Data', color ='C0')\n",
    "            plt.plot(valid_df.index, valid_df['Price'], label='Actual Data', color = 'C1')\n",
    "            plt.plot(valid_df.index, valid_df['Predictions'], label='Predictions', color ='C2')\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.title(f'MICN - {bank_name} (Ratio {int(train_ratio*10)}:{int((10 - train_ratio*10))}) - Next 90 Days')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiang\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m prediction_lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m30\u001b[39m]\n\u001b[0;32m      7\u001b[0m name \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIDV\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m \u001b[43mexport_forecast_by_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ratios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 113\u001b[0m, in \u001b[0;36mexport_forecast_by_datasets\u001b[1;34m(datasets, index_col, attribute, train_ratios, prediction_lengths, name)\u001b[0m\n\u001b[0;32m    111\u001b[0m last_date \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    112\u001b[0m color_plot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC4\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC5\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, pred_len \u001b[38;5;129;01min\u001b[39;00m prediction_lengths:\n\u001b[0;32m    114\u001b[0m     configs\u001b[38;5;241m.\u001b[39mpred_len \u001b[38;5;241m=\u001b[39m pred_len\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 768x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Thực thi model \n",
    "datasets = ['E:\\Year3\\Ser2\\DA_In_Bussiness\\Project_Code\\IS403_O21_16\\Model\\MICN\\dataset\\BID Historical Data.csv']\n",
    "index_col = 'Date'\n",
    "attribute = 'Price'\n",
    "train_ratios = [0.7, 0.8, 0.9]\n",
    "prediction_lengths = [90, 60, 30]\n",
    "name = ['BIDV']\n",
    "export_forecast_by_datasets(datasets, index_col, attribute, train_ratios, prediction_lengths ,name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
